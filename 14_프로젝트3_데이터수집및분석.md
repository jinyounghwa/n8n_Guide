# 14_프로젝트3: 데이터 수집 및 분석 자동화

웹사이트의 정보를 주기적으로 수집(Scraping)하고, 수집된 데이터를 분석하여 리포트를 생성하는 과정을 자동화합니다.

---

## 1. 프로젝트 개요
- **목적**: 경쟁사 모니터링이나 시장 트렌드 데이터 수집 자동화
- **사용 노드**: 
  - `Schedule`: 주기적 실행 (매일 오전 9시)
  - `HTTP Request`: 웹 페이지 HTML 가져오기
  - `HTML`: 특정 데이터(제목, 가격, 내용 등) 추출
  - `Code`: 데이터 가공 (JavaScript)
  - `Google Sheets`: 데이터 누적 저장 및 분석

---

## 2. 워크플로우 구성 단계

### 1단계: 주기적 실행 설정
- `Schedule` 노드를 사용하여 워크플로우가 자동으로 실행될 주기를 설정합니다.

### 2단계: 웹 스크래핑
- `HTTP Request` 노드로 목표 URL에 GET 요청을 보냅니다.
- `HTML` 노드에서 CSS Selector를 사용하여 수집할 요소(예: `.product-title`)를 지정합니다.

### 3단계: 데이터 클리닝
- `Code` 노드에서 수집된 데이터 중 오차가 있거나 빈 값이 있는 데이터를 정리합니다.
- 예: "15,000원" -> 15000 (숫자로 변환)

### 4단계: 시트 저장 및 Claude 분석
- `Google Sheets` 노드로 데이터를 한 줄씩 추가합니다.
- 최근 수집된 데이터의 경향성을 `Claude API`에 전달하여 요약 리포트를 작성합니다.

---

## 3. 학습 포인트
- 동적 데이터 수집의 기본 원리
- CSS Selector 활용법
- 비정형 데이터를 정형 데이터(Sheet)로 변환하는 기술

---

## 🔗 링크
- **[15_최종프로젝트.md](15_최종프로젝트.md)**: 마지막 프로젝트
- **[README.md](README.md)**: 메인 가이드로 돌아가기
